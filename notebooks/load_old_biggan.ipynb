{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intermediate-netherlands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "if '/home/cybai2020/BigGAN-PyTorch/' not in sys.path:\r\n",
      "    sys.path.insert(0, '/home/cybai2020/BigGAN-PyTorch/')\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "from tqdm import tqdm\r\n",
      "import functools\r\n",
      "from importlib import reload\r\n",
      "from matplotlib import pyplot as plt\r\n",
      "import scipy\r\n",
      "import os\r\n",
      "\r\n",
      "import torch\r\n",
      "from torch.nn import Parameter as P\r\n",
      "import torch.nn.functional as F\r\n",
      "from torchvision import datasets, transforms\r\n",
      "from torch.utils.data import DataLoader\r\n",
      "\r\n",
      "import inception_utils\r\n",
      "import utils\r\n",
      "\r\n",
      "def register_hook(model_name, cifar10_models_dict, outputs, to_cpu=True, cast_numpy=False):\r\n",
      "    def _hook(module, input, output):\r\n",
      "        output = torch.flatten(output, start_dim=1).detach()\r\n",
      "        if to_cpu:\r\n",
      "            output = output.cpu()\r\n",
      "        if cast_numpy:\r\n",
      "            output = output.numpy()\r\n",
      "        outputs.append(output)\r\n",
      "        \r\n",
      "    if 'vgg' in model_name:\r\n",
      "        handle = cifar10_models_dict[model_name].classifier[3].register_forward_hook(_hook)\r\n",
      "    elif 'resnet' in model_name or 'googlenet' in model_name:\r\n",
      "        handle = cifar10_models_dict[model_name].avgpool.register_forward_hook(_hook)\r\n",
      "    elif 'densenet' in model_name or 'mobilenet' in model_name:\r\n",
      "        handle = cifar10_models_dict[model_name].features.register_forward_hook(\r\n",
      "            lambda module, input, output: _hook(module, input, output.mean([2, 3])))\r\n",
      "    elif 'inception' in model_name and 'cifar10' in model_name:\r\n",
      "        handle = cifar10_models_dict[model_name].Mixed_7c.register_forward_hook(\r\n",
      "            lambda module, input, output: _hook(module, input, F.adaptive_avg_pool2d(output, 1)))\r\n",
      "    elif 'inception' in model_name and 'imagenet' in model_name:\r\n",
      "        handle = cifar10_models_dict[model_name].Mixed_7c.register_forward_hook(\r\n",
      "            lambda module, input, output: _hook(module, input, torch.mean(output.view(output.size(0), output.size(1), -1), 2)))\r\n",
      "    else:\r\n",
      "        raise NotImplementedError()\r\n",
      "    \r\n",
      "    return cifar10_models_dict[model_name], handle\r\n",
      "\r\n",
      "def parse_nnd_log(line):\r\n",
      "    _, t_str, nnd_str, _, time_str = line.split(' ')\r\n",
      "    t = float(t_str.split('=')[-1][:-1])\r\n",
      "    nnd = float(nnd_str.split('=')[-1][:-1])\r\n",
      "    time = float(time_str.split('=')[-1])\r\n",
      "    nnd_data = {'t': t, 'nnd': nnd, 'time': time}\r\n",
      "    return nnd_data\r\n",
      "\r\n",
      "def NN_test(T, Qm, device, bsize=50):\r\n",
      "    if T.shape[0] > Qm.shape[0]:\r\n",
      "        T = T[np.random.choice(T.shape[0], Qm.shape[0], replace=False)]\r\n",
      "    else:\r\n",
      "        Qm = Qm[:T.shape[0]]\r\n",
      "    # requires same size\r\n",
      "    cat_tensor = torch.from_numpy(np.concatenate([T, Qm], axis=0)).float().to(device)\r\n",
      "    cat_tensor = torch.div(cat_tensor, \r\n",
      "                           torch.norm(cat_tensor, dim=1).view(-1, 1))\r\n",
      "    \r\n",
      "    # T wrt Qm\r\n",
      "    n_correct, n_total = 0, 0\r\n",
      "    for bstart in range(0, T.shape[0], bsize):\r\n",
      "        with torch.no_grad():\r\n",
      "            T_tensor = cat_tensor[bstart:bstart + bsize]\r\n",
      "            _, mdi = compute_memorization_distance(T_tensor, cat_tensor, k=2)\r\n",
      "            n_correct += (mdi.cpu().numpy() < T.shape[0]).sum()\r\n",
      "            n_total += mdi.shape[0]\r\n",
      "    acc_T = n_correct / n_total\r\n",
      "    \r\n",
      "    # Qm wrt T\r\n",
      "    n_correct, n_total = 0, 0\r\n",
      "    for bstart in range(0, Qm.shape[0], bsize):\r\n",
      "        with torch.no_grad():\r\n",
      "            Qm_tensor = cat_tensor[T.shape[0] + bstart:T.shape[0] + bstart + bsize]\r\n",
      "            _, mdi = compute_memorization_distance(Qm_tensor, cat_tensor, k=2)\r\n",
      "            n_correct += (mdi.cpu().numpy() >= T.shape[0]).sum()\r\n",
      "            n_total += mdi.shape[0]\r\n",
      "    acc_Qm = n_correct / n_total\r\n",
      "    \r\n",
      "    return acc_T, acc_Qm\r\n",
      "\r\n",
      "def compute_memorization_distance(fake_features_tensor, real_normed_tensor, k=1, normalize=True):\r\n",
      "    if normalize:\r\n",
      "        fake_features_tensor = torch.div(fake_features_tensor, \r\n",
      "                                         torch.norm(fake_features_tensor, dim=1).view(-1, 1))\r\n",
      "    d = 1.0 - torch.abs(torch.mm(fake_features_tensor, real_normed_tensor.T))\r\n",
      "    val, idx = torch.topk(d, k, largest=False, dim=1)\r\n",
      "    return val[:, -1], idx[:, -1]\r\n",
      "\r\n",
      "def get_inception_metrics(data_mu, data_sigma, pool, logits, labels, num_splits=10):\r\n",
      "    IS_mean, IS_std = inception_utils.calculate_inception_score(logits, num_splits)\r\n",
      "    \r\n",
      "    mu, sigma = np.mean(pool, axis=0), np.cov(pool, rowvar=False)\r\n",
      "    FID = inception_utils.numpy_calculate_frechet_distance(mu, sigma, data_mu, data_sigma)\r\n",
      "    \r\n",
      "    return IS_mean, IS_std, FID\r\n",
      "\r\n",
      "def calculate_inception_moments(loaders, device, return_stats_only=True, parallel=False, class_weight=False):\r\n",
      "    # Load inception net\r\n",
      "    net = inception_utils.load_inception_net(parallel=parallel) # accepts [-1, 1]\r\n",
      "    pool, logits, labels = [], [], []\r\n",
      "    \r\n",
      "    for i, (x, y) in enumerate(tqdm(loaders[0])):\r\n",
      "        x = x.to(device)\r\n",
      "        with torch.no_grad():\r\n",
      "            pool_val, logits_val = net(x)\r\n",
      "            pool += [np.asarray(pool_val.cpu())]\r\n",
      "            logits += [np.asarray(F.softmax(logits_val, 1).cpu())]\r\n",
      "            labels += [np.asarray(y.cpu())]\r\n",
      "\r\n",
      "    pool, logits, labels = [np.concatenate(item, 0) for item in [pool, logits, labels]]\r\n",
      "    IS_mean, IS_std = inception_utils.calculate_inception_score(logits)\r\n",
      "    print(f\"IS stats = {IS_mean} +- {IS_std}\")\r\n",
      "    \r\n",
      "    weights = None\r\n",
      "    \r\n",
      "    if class_weight:\r\n",
      "        weights = np.empty_like(labels, dtype=np.float32)\r\n",
      "        for uclass, class_size in zip(*np.unique(labels, return_counts=True)):\r\n",
      "            weights[labels == uclass] = 1 / class_size\r\n",
      "    \r\n",
      "    mu = np.average(pool, axis=0, weights=weights)\r\n",
      "    sigma = np.cov(pool, rowvar=False, aweights=weights)\r\n",
      "    \r\n",
      "    if return_stats_only:\r\n",
      "        return {'mu': mu, 'sigma': sigma}\r\n",
      "    else:\r\n",
      "        return {'pool': pool, 'logits': logits, 'labels': labels}, {'mu': mu, 'sigma': sigma}\r\n",
      "\r\n",
      "def load_trained_sampler(experiment_name, device, weights_root='/home/cybai2020/BigGAN-PyTorch/weights', n_class=None, bsize=None):\r\n",
      "    # define params here\r\n",
      "    params = \"--shuffle --batch_size 50 --num_G_accumulations 1 --num_D_accumulations 1 --num_epochs 500 \\\r\n",
      "--num_D_steps 4 --G_lr 2e-4 --D_lr 2e-4 --dataset C10 --G_ortho 0.0 --G_attn 0 --D_attn 0 \\\r\n",
      "--G_init N02 --D_init N02 --ema --use_ema --ema_start 1000 --test_every 5000 --save_every 2000 \\\r\n",
      "--num_best_copies 5 --num_save_copies 2 --seed 0\"\r\n",
      "\r\n",
      "    parser = utils.prepare_parser()\r\n",
      "    config = vars(parser.parse_args(params.split(' ')))\r\n",
      "\r\n",
      "    config['resolution'] = utils.imsize_dict[config['dataset']]\r\n",
      "    config['n_classes'] = utils.nclass_dict[config['dataset']]\r\n",
      "    config['G_activation'] = utils.activation_dict[config['G_nl']]\r\n",
      "    config['D_activation'] = utils.activation_dict[config['D_nl']]\r\n",
      "    config['skip_init'] = True\r\n",
      "\r\n",
      "    if bsize is not None:\r\n",
      "        config['batch_size'] = bsize\r\n",
      "\r\n",
      "    config = utils.update_config_roots(config)\r\n",
      "    utils.seed_rng(config['seed'])\r\n",
      "    \r\n",
      "    # import trained generator\r\n",
      "    model = __import__(config['model'])\r\n",
      "    G = model.Generator(**config).to(device)\r\n",
      "    G_ema = model.Generator(**config).to(device)\r\n",
      "    \r\n",
      "    # load pretrained weights\r\n",
      "    utils.load_weights(G, D=None, state_dict={}, weights_root=weights_root, \r\n",
      "                       experiment_name=experiment_name, name_suffix=None, \r\n",
      "                       G_ema=G_ema, strict=True, load_optim=False)\r\n",
      "    G_batch_size = max(config['G_batch_size'], config['batch_size'])\r\n",
      "    z_, y_ = utils.prepare_z_y(G_batch_size, G.dim_z, config['n_classes'],\r\n",
      "                               device=device, fp16=config['G_fp16'])\r\n",
      "\r\n",
      "    if n_class is not None:\r\n",
      "        y_ = utils.Distribution(torch.zeros(G_batch_size, requires_grad=False))\r\n",
      "        y_.init_distribution('constant', n_class=n_class)\r\n",
      "        y_ = y_.to(device, torch.int64)\r\n",
      "\r\n",
      "    sample = functools.partial(utils.sample,\r\n",
      "                               G=(G_ema if config['ema'] and config['use_ema'] else G),\r\n",
      "                               z_=z_, y_=y_, config=config)\r\n",
      "    return sample\r\n",
      "\r\n",
      "class MaskedDataset(torch.utils.data.Dataset):\r\n",
      "    def __init__(self, dset, mask):\r\n",
      "        assert(len(dset) == len(mask))\r\n",
      "        self.dset = dset\r\n",
      "        self.translate_index = np.arange(len(mask)).astype(np.int)[mask]\r\n",
      "    def __len__(self):\r\n",
      "        return len(self.translate_index)\r\n",
      "    def __getitem__(self, idx):\r\n",
      "        return self.dset[self.translate_index[idx]]\r\n",
      "\r\n",
      "def autolabel(rects, ax, n_digits=2):\r\n",
      "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\r\n",
      "    for rect in rects:\r\n",
      "        height = rect.get_height()\r\n",
      "        ax.annotate(f'{height:.{n_digits}f}',\r\n",
      "                    xy=(rect.get_x() + rect.get_width() / 2, height),\r\n",
      "                    xytext=(0, 3 if height > 0 else -12),  # 3 points vertical offset\r\n",
      "                    textcoords=\"offset points\",\r\n",
      "                    ha='center', va='bottom')\r\n"
     ]
    }
   ],
   "source": [
    "%cat /home/cybai/BigGAN-PyTorch/notebooks/misc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "geographic-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "# -*- coding: utf-8 -*-\r\n",
      "\r\n",
      "''' Utilities file\r\n",
      "This file contains utility functions for bookkeeping, logging, and data loading.\r\n",
      "Methods which directly affect training should either go in layers, the model,\r\n",
      "or train_fns.py.\r\n",
      "'''\r\n",
      "\r\n",
      "from __future__ import print_function\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import numpy as np\r\n",
      "import time\r\n",
      "import datetime\r\n",
      "import json\r\n",
      "import pickle\r\n",
      "from argparse import ArgumentParser\r\n",
      "import animal_hash\r\n",
      "\r\n",
      "import torch\r\n",
      "import torch.nn as nn\r\n",
      "import torch.nn.functional as F\r\n",
      "import torchvision\r\n",
      "import torchvision.transforms as transforms\r\n",
      "from torch.utils.data import DataLoader\r\n",
      "\r\n",
      "import datasets as dset\r\n",
      "\r\n",
      "def prepare_parser():\r\n",
      "  usage = 'Parser for all scripts.'\r\n",
      "  parser = ArgumentParser(description=usage)\r\n",
      "\r\n",
      "  ### Memorization rejection stuff ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--mrt', type=float, default=0.0,\r\n",
      "    help='Memorization rejection threshold: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--mrq', type=float, default=None,\r\n",
      "    help='Memorization rejection quantile: %(default)s)')\r\n",
      "  \r\n",
      "  ### Dataset/Dataloader stuff ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--dataset', type=str, default='I128_hdf5',\r\n",
      "    help='Which Dataset to train on, out of I128, I256, C10, C100;'\r\n",
      "         'Append \"_hdf5\" to use the hdf5 version for ISLVRC '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--augment', action='store_true', default=False,\r\n",
      "    help='Augment with random crops and flips (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_workers', type=int, default=8,\r\n",
      "    help='Number of dataloader workers; consider using less for HDF5 '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--no_pin_memory', action='store_false', dest='pin_memory', default=True,\r\n",
      "    help='Pin data into memory through dataloader? (default: %(default)s)') \r\n",
      "  parser.add_argument(\r\n",
      "    '--shuffle', action='store_true', default=False,\r\n",
      "    help='Shuffle the data (strongly recommended)? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--load_in_mem', action='store_true', default=False,\r\n",
      "    help='Load all data into memory? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--use_multiepoch_sampler', action='store_true', default=False,\r\n",
      "    help='Use the multi-epoch sampler for dataloader? (default: %(default)s)')\r\n",
      "  \r\n",
      "  \r\n",
      "  ### Model stuff ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--model', type=str, default='BigGAN',\r\n",
      "    help='Name of the model module (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_param', type=str, default='SN',\r\n",
      "    help='Parameterization style to use for G, spectral norm (SN) or SVD (SVD)'\r\n",
      "          ' or None (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_param', type=str, default='SN',\r\n",
      "    help='Parameterization style to use for D, spectral norm (SN) or SVD (SVD)'\r\n",
      "         ' or None (default: %(default)s)')    \r\n",
      "  parser.add_argument(\r\n",
      "    '--G_ch', type=int, default=64,\r\n",
      "    help='Channel multiplier for G (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_ch', type=int, default=64,\r\n",
      "    help='Channel multiplier for D (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_depth', type=int, default=1,\r\n",
      "    help='Number of resblocks per stage in G? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_depth', type=int, default=1,\r\n",
      "    help='Number of resblocks per stage in D? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_thin', action='store_false', dest='D_wide', default=True,\r\n",
      "    help='Use the SN-GAN channel pattern for D? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_shared', action='store_true', default=False,\r\n",
      "    help='Use shared embeddings in G? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--shared_dim', type=int, default=0,\r\n",
      "    help='G''s shared embedding dimensionality; if 0, will be equal to dim_z. '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--dim_z', type=int, default=128,\r\n",
      "    help='Noise dimensionality: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--z_var', type=float, default=1.0,\r\n",
      "    help='Noise variance: %(default)s)')    \r\n",
      "  parser.add_argument(\r\n",
      "    '--hier', action='store_true', default=False,\r\n",
      "    help='Use hierarchical z in G? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--cross_replica', action='store_true', default=False,\r\n",
      "    help='Cross_replica batchnorm in G?(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--mybn', action='store_true', default=False,\r\n",
      "    help='Use my batchnorm (which supports standing stats?) %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_nl', type=str, default='relu',\r\n",
      "    help='Activation function for G (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_nl', type=str, default='relu',\r\n",
      "    help='Activation function for D (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_attn', type=str, default='64',\r\n",
      "    help='What resolutions to use attention on for G (underscore separated) '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_attn', type=str, default='64',\r\n",
      "    help='What resolutions to use attention on for D (underscore separated) '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--norm_style', type=str, default='bn',\r\n",
      "    help='Normalizer style for G, one of bn [batchnorm], in [instancenorm], '\r\n",
      "         'ln [layernorm], gn [groupnorm] (default: %(default)s)')\r\n",
      "         \r\n",
      "  ### Model init stuff ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--seed', type=int, default=0,\r\n",
      "    help='Random seed to use; affects both initialization and '\r\n",
      "         ' dataloading. (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_init', type=str, default='ortho',\r\n",
      "    help='Init style to use for G (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_init', type=str, default='ortho',\r\n",
      "    help='Init style to use for D(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--skip_init', action='store_true', default=False,\r\n",
      "    help='Skip initialization, ideal for testing when ortho init was used '\r\n",
      "          '(default: %(default)s)')\r\n",
      "  \r\n",
      "  ### Optimizer stuff ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_lr', type=float, default=5e-5,\r\n",
      "    help='Learning rate to use for Generator (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_lr', type=float, default=2e-4,\r\n",
      "    help='Learning rate to use for Discriminator (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_B1', type=float, default=0.0,\r\n",
      "    help='Beta1 to use for Generator (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_B1', type=float, default=0.0,\r\n",
      "    help='Beta1 to use for Discriminator (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_B2', type=float, default=0.999,\r\n",
      "    help='Beta2 to use for Generator (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_B2', type=float, default=0.999,\r\n",
      "    help='Beta2 to use for Discriminator (default: %(default)s)')\r\n",
      "    \r\n",
      "  ### Batch size, parallel, and precision stuff ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--batch_size', type=int, default=64,\r\n",
      "    help='Default overall batchsize (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_batch_size', type=int, default=0,\r\n",
      "    help='Batch size to use for G; if 0, same as D (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_G_accumulations', type=int, default=1,\r\n",
      "    help='Number of passes to accumulate G''s gradients over '\r\n",
      "         '(default: %(default)s)')  \r\n",
      "  parser.add_argument(\r\n",
      "    '--num_D_steps', type=int, default=2,\r\n",
      "    help='Number of D steps per G step (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_D_accumulations', type=int, default=1,\r\n",
      "    help='Number of passes to accumulate D''s gradients over '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--split_D', action='store_true', default=False,\r\n",
      "    help='Run D twice rather than concatenating inputs? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_epochs', type=int, default=100,\r\n",
      "    help='Number of epochs to train for (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--parallel', action='store_true', default=False,\r\n",
      "    help='Train with multiple GPUs (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_fp16', action='store_true', default=False,\r\n",
      "    help='Train with half-precision in G? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_fp16', action='store_true', default=False,\r\n",
      "    help='Train with half-precision in D? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_mixed_precision', action='store_true', default=False,\r\n",
      "    help='Train with half-precision activations but fp32 params in D? '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--G_mixed_precision', action='store_true', default=False,\r\n",
      "    help='Train with half-precision activations but fp32 params in G? '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--accumulate_stats', action='store_true', default=False,\r\n",
      "    help='Accumulate \"standing\" batchnorm stats? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_standing_accumulations', type=int, default=16,\r\n",
      "    help='Number of forward passes to use in accumulating standing stats? '\r\n",
      "         '(default: %(default)s)')        \r\n",
      "    \r\n",
      "  ### Bookkeping stuff ###  \r\n",
      "  parser.add_argument(\r\n",
      "    '--G_eval_mode', action='store_true', default=False,\r\n",
      "    help='Run G in eval mode (running/standing stats?) at sample/test time? '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--save_every', type=int, default=2000,\r\n",
      "    help='Save every X iterations (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_save_copies', type=int, default=2,\r\n",
      "    help='How many copies to save (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_best_copies', type=int, default=2,\r\n",
      "    help='How many previous best checkpoints to save (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--which_best', type=str, default='IS',\r\n",
      "    help='Which metric to use to determine when to save new \"best\"'\r\n",
      "         'checkpoints, one of IS or FID (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--no_fid', action='store_true', default=False,\r\n",
      "    help='Calculate IS only, not FID? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--test_every', type=int, default=5000,\r\n",
      "    help='Test every X iterations (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_inception_images', type=int, default=50000,\r\n",
      "    help='Number of samples to compute inception metrics with '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--hashname', action='store_true', default=False,\r\n",
      "    help='Use a hash of the experiment name instead of the full config '\r\n",
      "         '(default: %(default)s)') \r\n",
      "  parser.add_argument(\r\n",
      "    '--base_root', type=str, default='',\r\n",
      "    help='Default location to store all weights, samples, data, and logs '\r\n",
      "           ' (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--data_root', type=str, default='data',\r\n",
      "    help='Default location where data is stored (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--weights_root', type=str, default='weights',\r\n",
      "    help='Default location to store weights (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--logs_root', type=str, default='logs',\r\n",
      "    help='Default location to store logs (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--samples_root', type=str, default='samples',\r\n",
      "    help='Default location to store samples (default: %(default)s)')  \r\n",
      "  parser.add_argument(\r\n",
      "    '--pbar', type=str, default='mine',\r\n",
      "    help='Type of progressbar to use; one of \"mine\" or \"tqdm\" '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--name_suffix', type=str, default='',\r\n",
      "    help='Suffix for experiment name for loading weights for sampling '\r\n",
      "         '(consider \"best0\") (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--experiment_name', type=str, default='',\r\n",
      "    help='Optionally override the automatic experiment naming with this arg. '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--config_from_name', action='store_true', default=False,\r\n",
      "    help='Use a hash of the experiment name instead of the full config '\r\n",
      "         '(default: %(default)s)')\r\n",
      "         \r\n",
      "  ### EMA Stuff ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--ema', action='store_true', default=False,\r\n",
      "    help='Keep an ema of G''s weights? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--ema_decay', type=float, default=0.9999,\r\n",
      "    help='EMA decay rate (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--use_ema', action='store_true', default=False,\r\n",
      "    help='Use the EMA parameters of G for evaluation? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--ema_start', type=int, default=0,\r\n",
      "    help='When to start updating the EMA weights (default: %(default)s)')\r\n",
      "  \r\n",
      "  ### Numerical precision and SV stuff ### \r\n",
      "  parser.add_argument(\r\n",
      "    '--adam_eps', type=float, default=1e-8,\r\n",
      "    help='epsilon value to use for Adam (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--BN_eps', type=float, default=1e-5,\r\n",
      "    help='epsilon value to use for BatchNorm (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--SN_eps', type=float, default=1e-8,\r\n",
      "    help='epsilon value to use for Spectral Norm(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_G_SVs', type=int, default=1,\r\n",
      "    help='Number of SVs to track in G (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_D_SVs', type=int, default=1,\r\n",
      "    help='Number of SVs to track in D (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_G_SV_itrs', type=int, default=1,\r\n",
      "    help='Number of SV itrs in G (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--num_D_SV_itrs', type=int, default=1,\r\n",
      "    help='Number of SV itrs in D (default: %(default)s)')\r\n",
      "  \r\n",
      "  ### Ortho reg stuff ### \r\n",
      "  parser.add_argument(\r\n",
      "    '--G_ortho', type=float, default=0.0, # 1e-4 is default for BigGAN\r\n",
      "    help='Modified ortho reg coefficient in G(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--D_ortho', type=float, default=0.0,\r\n",
      "    help='Modified ortho reg coefficient in D (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--toggle_grads', action='store_true', default=True,\r\n",
      "    help='Toggle D and G''s \"requires_grad\" settings when not training them? '\r\n",
      "         ' (default: %(default)s)')\r\n",
      "  \r\n",
      "  ### Which train function ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--which_train_fn', type=str, default='GAN',\r\n",
      "    help='How2trainyourbois (default: %(default)s)')  \r\n",
      "  \r\n",
      "  ### Resume training stuff\r\n",
      "  parser.add_argument(\r\n",
      "    '--load_weights', type=str, default='',\r\n",
      "    help='Suffix for which weights to load (e.g. best0, copy0) '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--resume', action='store_true', default=False,\r\n",
      "    help='Resume training? (default: %(default)s)')\r\n",
      "  \r\n",
      "  ### Log stuff ###\r\n",
      "  parser.add_argument(\r\n",
      "    '--logstyle', type=str, default='%3.3e',\r\n",
      "    help='What style to use when logging training metrics?'\r\n",
      "         'One of: %#.#f/ %#.#e (float/exp, text),'\r\n",
      "         'pickle (python pickle),'\r\n",
      "         'npz (numpy zip),'\r\n",
      "         'mat (MATLAB .mat file) (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--log_G_spectra', action='store_true', default=False,\r\n",
      "    help='Log the top 3 singular values in each SN layer in G? '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--log_D_spectra', action='store_true', default=False,\r\n",
      "    help='Log the top 3 singular values in each SN layer in D? '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--sv_log_interval', type=int, default=10,\r\n",
      "    help='Iteration interval for logging singular values '\r\n",
      "         ' (default: %(default)s)') \r\n",
      "   \r\n",
      "  return parser\r\n",
      "\r\n",
      "# Arguments for sample.py; not presently used in train.py\r\n",
      "def add_sample_parser(parser):\r\n",
      "  parser.add_argument(\r\n",
      "    '--sample_npz', action='store_true', default=False,\r\n",
      "    help='Sample \"sample_num_npz\" images and save to npz? '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--sample_num_npz', type=int, default=50000,\r\n",
      "    help='Number of images to sample when sampling NPZs '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--sample_sheets', action='store_true', default=False,\r\n",
      "    help='Produce class-conditional sample sheets and stick them in '\r\n",
      "         'the samples root? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--sample_interps', action='store_true', default=False,\r\n",
      "    help='Produce interpolation sheets and stick them in '\r\n",
      "         'the samples root? (default: %(default)s)')         \r\n",
      "  parser.add_argument(\r\n",
      "    '--sample_sheet_folder_num', type=int, default=-1,\r\n",
      "    help='Number to use for the folder for these sample sheets '\r\n",
      "         '(default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--sample_random', action='store_true', default=False,\r\n",
      "    help='Produce a single random sheet? (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--sample_trunc_curves', type=str, default='',\r\n",
      "    help='Get inception metrics with a range of variances?'\r\n",
      "         'To use this, specify a startpoint, step, and endpoint, e.g. '\r\n",
      "         '--sample_trunc_curves 0.2_0.1_1.0 for a startpoint of 0.2, '\r\n",
      "         'endpoint of 1.0, and stepsize of 1.0.  Note that this is '\r\n",
      "         'not exactly identical to using tf.truncated_normal, but should '\r\n",
      "         'have approximately the same effect. (default: %(default)s)')\r\n",
      "  parser.add_argument(\r\n",
      "    '--sample_inception_metrics', action='store_true', default=False,\r\n",
      "    help='Calculate Inception metrics with sample.py? (default: %(default)s)')  \r\n",
      "  return parser\r\n",
      "\r\n",
      "# Convenience dicts\r\n",
      "dset_dict = {'I32': dset.ImageFolder, 'I64': dset.ImageFolder, \r\n",
      "             'I128': dset.ImageFolder, 'I256': dset.ImageFolder,\r\n",
      "             'I32_hdf5': dset.ILSVRC_HDF5, 'I64_hdf5': dset.ILSVRC_HDF5, \r\n",
      "             'I128_hdf5': dset.ILSVRC_HDF5, 'I256_hdf5': dset.ILSVRC_HDF5,\r\n",
      "             'C10': dset.CIFAR10, 'C100': dset.CIFAR100}\r\n",
      "imsize_dict = {'I32': 32, 'I32_hdf5': 32,\r\n",
      "               'I64': 64, 'I64_hdf5': 64,\r\n",
      "               'I128': 128, 'I128_hdf5': 128,\r\n",
      "               'I256': 256, 'I256_hdf5': 256,\r\n",
      "               'C10': 32, 'C100': 32}\r\n",
      "root_dict = {'I32': 'ImageNet', 'I32_hdf5': 'ILSVRC32.hdf5',\r\n",
      "             'I64': 'ImageNet', 'I64_hdf5': 'ILSVRC64.hdf5',\r\n",
      "             'I128': 'ImageNet', 'I128_hdf5': 'ILSVRC128.hdf5',\r\n",
      "             'I256': 'ImageNet', 'I256_hdf5': 'ILSVRC256.hdf5',\r\n",
      "             'C10': 'cifar', 'C100': 'cifar'}\r\n",
      "nclass_dict = {'I32': 1000, 'I32_hdf5': 1000,\r\n",
      "               'I64': 1000, 'I64_hdf5': 1000,\r\n",
      "               'I128': 1000, 'I128_hdf5': 1000,\r\n",
      "               'I256': 1000, 'I256_hdf5': 1000,\r\n",
      "               'C10': 10, 'C100': 100}\r\n",
      "# Number of classes to put per sample sheet               \r\n",
      "classes_per_sheet_dict = {'I32': 50, 'I32_hdf5': 50,\r\n",
      "                          'I64': 50, 'I64_hdf5': 50,\r\n",
      "                          'I128': 20, 'I128_hdf5': 20,\r\n",
      "                          'I256': 20, 'I256_hdf5': 20,\r\n",
      "                          'C10': 10, 'C100': 100}\r\n",
      "activation_dict = {'inplace_relu': nn.ReLU(inplace=True),\r\n",
      "                   'relu': nn.ReLU(inplace=False),\r\n",
      "                   'ir': nn.ReLU(inplace=True),}\r\n",
      "\r\n",
      "class CenterCropLongEdge(object):\r\n",
      "  \"\"\"Crops the given PIL Image on the long edge.\r\n",
      "  Args:\r\n",
      "      size (sequence or int): Desired output size of the crop. If size is an\r\n",
      "          int instead of sequence like (h, w), a square crop (size, size) is\r\n",
      "          made.\r\n",
      "  \"\"\"\r\n",
      "  def __call__(self, img):\r\n",
      "    \"\"\"\r\n",
      "    Args:\r\n",
      "        img (PIL Image): Image to be cropped.\r\n",
      "    Returns:\r\n",
      "        PIL Image: Cropped image.\r\n",
      "    \"\"\"\r\n",
      "    return transforms.functional.center_crop(img, min(img.size))\r\n",
      "\r\n",
      "  def __repr__(self):\r\n",
      "    return self.__class__.__name__\r\n",
      "\r\n",
      "class RandomCropLongEdge(object):\r\n",
      "  \"\"\"Crops the given PIL Image on the long edge with a random start point.\r\n",
      "  Args:\r\n",
      "      size (sequence or int): Desired output size of the crop. If size is an\r\n",
      "          int instead of sequence like (h, w), a square crop (size, size) is\r\n",
      "          made.\r\n",
      "  \"\"\"\r\n",
      "  def __call__(self, img):\r\n",
      "    \"\"\"\r\n",
      "    Args:\r\n",
      "        img (PIL Image): Image to be cropped.\r\n",
      "    Returns:\r\n",
      "        PIL Image: Cropped image.\r\n",
      "    \"\"\"\r\n",
      "    size = (min(img.size), min(img.size))\r\n",
      "    # Only step forward along this edge if it's the long edge\r\n",
      "    i = (0 if size[0] == img.size[0] \r\n",
      "          else np.random.randint(low=0,high=img.size[0] - size[0]))\r\n",
      "    j = (0 if size[1] == img.size[1]\r\n",
      "          else np.random.randint(low=0,high=img.size[1] - size[1]))\r\n",
      "    return transforms.functional.crop(img, i, j, size[0], size[1])\r\n",
      "\r\n",
      "  def __repr__(self):\r\n",
      "    return self.__class__.__name__\r\n",
      "\r\n",
      "\r\n",
      "# multi-epoch Dataset sampler to avoid memory leakage and enable resumption of\r\n",
      "# training from the same sample regardless of if we stop mid-epoch\r\n",
      "class MultiEpochSampler(torch.utils.data.Sampler):\r\n",
      "  r\"\"\"Samples elements randomly over multiple epochs\r\n",
      "\r\n",
      "  Arguments:\r\n",
      "      data_source (Dataset): dataset to sample from\r\n",
      "      num_epochs (int) : Number of times to loop over the dataset\r\n",
      "      start_itr (int) : which iteration to begin from\r\n",
      "  \"\"\"\r\n",
      "\r\n",
      "  def __init__(self, data_source, num_epochs, start_itr=0, batch_size=128):\r\n",
      "    self.data_source = data_source\r\n",
      "    self.num_samples = len(self.data_source)\r\n",
      "    self.num_epochs = num_epochs\r\n",
      "    self.start_itr = start_itr\r\n",
      "    self.batch_size = batch_size\r\n",
      "\r\n",
      "    if not isinstance(self.num_samples, int) or self.num_samples <= 0:\r\n",
      "      raise ValueError(\"num_samples should be a positive integeral \"\r\n",
      "                       \"value, but got num_samples={}\".format(self.num_samples))\r\n",
      "\r\n",
      "  def __iter__(self):\r\n",
      "    n = len(self.data_source)\r\n",
      "    # Determine number of epochs\r\n",
      "    num_epochs = int(np.ceil((n * self.num_epochs \r\n",
      "                              - (self.start_itr * self.batch_size)) / float(n)))\r\n",
      "    # Sample all the indices, and then grab the last num_epochs index sets;\r\n",
      "    # This ensures if we're starting at epoch 4, we're still grabbing epoch 4's\r\n",
      "    # indices\r\n",
      "    out = [torch.randperm(n) for epoch in range(self.num_epochs)][-num_epochs:]\r\n",
      "    # Ignore the first start_itr % n indices of the first epoch\r\n",
      "    out[0] = out[0][(self.start_itr * self.batch_size % n):]\r\n",
      "    # if self.replacement:\r\n",
      "      # return iter(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())\r\n",
      "    # return iter(.tolist())\r\n",
      "    output = torch.cat(out).tolist()\r\n",
      "    print('Length dataset output is %d' % len(output))\r\n",
      "    return iter(output)\r\n",
      "\r\n",
      "  def __len__(self):\r\n",
      "    return len(self.data_source) * self.num_epochs - self.start_itr * self.batch_size\r\n",
      "\r\n",
      "\r\n",
      "# Convenience function to centralize all data loaders\r\n",
      "def get_data_loaders(dataset, data_root=None, augment=False, batch_size=64, \r\n",
      "                     num_workers=8, shuffle=True, load_in_mem=False, hdf5=False,\r\n",
      "                     pin_memory=True, drop_last=True, start_itr=0,\r\n",
      "                     num_epochs=500, use_multiepoch_sampler=False,\r\n",
      "                     **kwargs):\r\n",
      "\r\n",
      "  # Append /FILENAME.hdf5 to root if using hdf5\r\n",
      "  data_root += '/%s' % root_dict[dataset]\r\n",
      "  print('Using dataset root location %s' % data_root)\r\n",
      "\r\n",
      "  which_dataset = dset_dict[dataset]\r\n",
      "  norm_mean = [0.5,0.5,0.5]\r\n",
      "  norm_std = [0.5,0.5,0.5]\r\n",
      "  image_size = imsize_dict[dataset]\r\n",
      "  # For image folder datasets, name of the file where we store the precomputed\r\n",
      "  # image locations to avoid having to walk the dirs every time we load.\r\n",
      "  dataset_kwargs = {'index_filename': '%s_imgs.npz' % dataset}\r\n",
      "  \r\n",
      "  # HDF5 datasets have their own inbuilt transform, no need to train_transform  \r\n",
      "  if 'hdf5' in dataset:\r\n",
      "    train_transform = None\r\n",
      "  else:\r\n",
      "    if augment:\r\n",
      "      print('Data will be augmented...')\r\n",
      "      if dataset in ['C10', 'C100']:\r\n",
      "        train_transform = [transforms.RandomCrop(32, padding=4),\r\n",
      "                           transforms.RandomHorizontalFlip()]\r\n",
      "      else:\r\n",
      "        train_transform = [RandomCropLongEdge(),\r\n",
      "                         transforms.Resize(image_size),\r\n",
      "                         transforms.RandomHorizontalFlip()]\r\n",
      "    else:\r\n",
      "      print('Data will not be augmented...')\r\n",
      "      if dataset in ['C10', 'C100']:\r\n",
      "        train_transform = []\r\n",
      "      else:\r\n",
      "        train_transform = [CenterCropLongEdge(), transforms.Resize(image_size)]\r\n",
      "      # train_transform = [transforms.Resize(image_size), transforms.CenterCrop]\r\n",
      "    train_transform = transforms.Compose(train_transform + [\r\n",
      "                     transforms.ToTensor(),\r\n",
      "                     transforms.Normalize(norm_mean, norm_std)])\r\n",
      "  train_set = which_dataset(root=data_root, transform=train_transform,\r\n",
      "                            load_in_mem=load_in_mem, **dataset_kwargs)\r\n",
      "\r\n",
      "  # Prepare loader; the loaders list is for forward compatibility with\r\n",
      "  # using validation / test splits.\r\n",
      "  loaders = []   \r\n",
      "  if use_multiepoch_sampler:\r\n",
      "    print('Using multiepoch sampler from start_itr %d...' % start_itr)\r\n",
      "    loader_kwargs = {'num_workers': num_workers, 'pin_memory': pin_memory}\r\n",
      "    sampler = MultiEpochSampler(train_set, num_epochs, start_itr, batch_size)\r\n",
      "    train_loader = DataLoader(train_set, batch_size=batch_size,\r\n",
      "                              sampler=sampler, **loader_kwargs)\r\n",
      "  else:\r\n",
      "    loader_kwargs = {'num_workers': num_workers, 'pin_memory': pin_memory,\r\n",
      "                     'drop_last': drop_last} # Default, drop last incomplete batch\r\n",
      "    train_loader = DataLoader(train_set, batch_size=batch_size,\r\n",
      "                              shuffle=shuffle, **loader_kwargs)\r\n",
      "  loaders.append(train_loader)\r\n",
      "  return loaders\r\n",
      "\r\n",
      "\r\n",
      "# Utility file to seed rngs\r\n",
      "def seed_rng(seed):\r\n",
      "  torch.manual_seed(seed)\r\n",
      "  torch.cuda.manual_seed(seed)\r\n",
      "  np.random.seed(seed)\r\n",
      "\r\n",
      "\r\n",
      "# Utility to peg all roots to a base root\r\n",
      "# If a base root folder is provided, peg all other root folders to it.\r\n",
      "def update_config_roots(config):\r\n",
      "  if config['base_root']:\r\n",
      "    print('Pegging all root folders to base root %s' % config['base_root'])\r\n",
      "    for key in ['data', 'weights', 'logs', 'samples']:\r\n",
      "      config['%s_root' % key] = '%s/%s' % (config['base_root'], key)\r\n",
      "  return config\r\n",
      "\r\n",
      "\r\n",
      "# Utility to prepare root folders if they don't exist; parent folder must exist\r\n",
      "def prepare_root(config):\r\n",
      "  for key in ['weights_root', 'logs_root', 'samples_root']:\r\n",
      "    if not os.path.exists(config[key]):\r\n",
      "      print('Making directory %s for %s...' % (config[key], key))\r\n",
      "      os.mkdir(config[key])\r\n",
      "\r\n",
      "\r\n",
      "# Simple wrapper that applies EMA to a model. COuld be better done in 1.0 using\r\n",
      "# the parameters() and buffers() module functions, but for now this works\r\n",
      "# with state_dicts using .copy_\r\n",
      "class ema(object):\r\n",
      "  def __init__(self, source, target, decay=0.9999, start_itr=0):\r\n",
      "    self.source = source\r\n",
      "    self.target = target\r\n",
      "    self.decay = decay\r\n",
      "    # Optional parameter indicating what iteration to start the decay at\r\n",
      "    self.start_itr = start_itr\r\n",
      "    # Initialize target's params to be source's\r\n",
      "    self.source_dict = self.source.state_dict()\r\n",
      "    self.target_dict = self.target.state_dict()\r\n",
      "    print('Initializing EMA parameters to be source parameters...')\r\n",
      "    with torch.no_grad():\r\n",
      "      for key in self.source_dict:\r\n",
      "        self.target_dict[key].data.copy_(self.source_dict[key].data)\r\n",
      "        # target_dict[key].data = source_dict[key].data # Doesn't work!\r\n",
      "\r\n",
      "  def update(self, itr=None):\r\n",
      "    # If an iteration counter is provided and itr is less than the start itr,\r\n",
      "    # peg the ema weights to the underlying weights.\r\n",
      "    if itr and itr < self.start_itr:\r\n",
      "      decay = 0.0\r\n",
      "    else:\r\n",
      "      decay = self.decay\r\n",
      "    with torch.no_grad():\r\n",
      "      for key in self.source_dict:\r\n",
      "        self.target_dict[key].data.copy_(self.target_dict[key].data * decay \r\n",
      "                                     + self.source_dict[key].data * (1 - decay))\r\n",
      "\r\n",
      "\r\n",
      "# Apply modified ortho reg to a model\r\n",
      "# This function is an optimized version that directly computes the gradient,\r\n",
      "# instead of computing and then differentiating the loss.\r\n",
      "def ortho(model, strength=1e-4, blacklist=[]):\r\n",
      "  with torch.no_grad():\r\n",
      "    for param in model.parameters():\r\n",
      "      # Only apply this to parameters with at least 2 axes, and not in the blacklist\r\n",
      "      if len(param.shape) < 2 or any([param is item for item in blacklist]):\r\n",
      "        continue\r\n",
      "      w = param.view(param.shape[0], -1)\r\n",
      "      grad = (2 * torch.mm(torch.mm(w, w.t()) \r\n",
      "              * (1. - torch.eye(w.shape[0], device=w.device)), w))\r\n",
      "      param.grad.data += strength * grad.view(param.shape)\r\n",
      "\r\n",
      "\r\n",
      "# Default ortho reg\r\n",
      "# This function is an optimized version that directly computes the gradient,\r\n",
      "# instead of computing and then differentiating the loss.\r\n",
      "def default_ortho(model, strength=1e-4, blacklist=[]):\r\n",
      "  with torch.no_grad():\r\n",
      "    for param in model.parameters():\r\n",
      "      # Only apply this to parameters with at least 2 axes & not in blacklist\r\n",
      "      if len(param.shape) < 2 or param in blacklist:\r\n",
      "        continue\r\n",
      "      w = param.view(param.shape[0], -1)\r\n",
      "      grad = (2 * torch.mm(torch.mm(w, w.t()) \r\n",
      "               - torch.eye(w.shape[0], device=w.device), w))\r\n",
      "      param.grad.data += strength * grad.view(param.shape)\r\n",
      "\r\n",
      "\r\n",
      "# Convenience utility to switch off requires_grad\r\n",
      "def toggle_grad(model, on_or_off):\r\n",
      "  for param in model.parameters():\r\n",
      "    param.requires_grad = on_or_off\r\n",
      "\r\n",
      "\r\n",
      "# Function to join strings or ignore them\r\n",
      "# Base string is the string to link \"strings,\" while strings\r\n",
      "# is a list of strings or Nones.\r\n",
      "def join_strings(base_string, strings):\r\n",
      "  return base_string.join([item for item in strings if item])\r\n",
      "\r\n",
      "\r\n",
      "# Save a model's weights, optimizer, and the state_dict\r\n",
      "def save_weights(G, D, state_dict, weights_root, experiment_name, \r\n",
      "                 name_suffix=None, G_ema=None):\r\n",
      "  root = '/'.join([weights_root, experiment_name])\r\n",
      "  if not os.path.exists(root):\r\n",
      "    os.mkdir(root)\r\n",
      "  if name_suffix:\r\n",
      "    print('Saving weights to %s/%s...' % (root, name_suffix))\r\n",
      "  else:\r\n",
      "    print('Saving weights to %s...' % root)\r\n",
      "  torch.save(G.state_dict(), \r\n",
      "              '%s/%s.pth' % (root, join_strings('_', ['G', name_suffix])))\r\n",
      "  torch.save(G.optim.state_dict(), \r\n",
      "              '%s/%s.pth' % (root, join_strings('_', ['G_optim', name_suffix])))\r\n",
      "  torch.save(D.state_dict(), \r\n",
      "              '%s/%s.pth' % (root, join_strings('_', ['D', name_suffix])))\r\n",
      "  torch.save(D.optim.state_dict(),\r\n",
      "              '%s/%s.pth' % (root, join_strings('_', ['D_optim', name_suffix])))\r\n",
      "  torch.save(state_dict,\r\n",
      "              '%s/%s.pth' % (root, join_strings('_', ['state_dict', name_suffix])))\r\n",
      "  if G_ema is not None:\r\n",
      "    torch.save(G_ema.state_dict(), \r\n",
      "                '%s/%s.pth' % (root, join_strings('_', ['G_ema', name_suffix])))\r\n",
      "\r\n",
      "\r\n",
      "# Load a model's weights, optimizer, and the state_dict\r\n",
      "def load_weights(G, D, state_dict, weights_root, experiment_name, \r\n",
      "                 name_suffix=None, G_ema=None, strict=True, load_optim=True):\r\n",
      "  root = '/'.join([weights_root, experiment_name])\r\n",
      "  if name_suffix:\r\n",
      "    print('Loading %s weights from %s...' % (name_suffix, root))\r\n",
      "  else:\r\n",
      "    print('Loading weights from %s...' % root)\r\n",
      "  if G is not None:\r\n",
      "    G.load_state_dict(\r\n",
      "      torch.load('%s/%s.pth' % (root, join_strings('_', ['G', name_suffix]))),\r\n",
      "      strict=strict)\r\n",
      "    if load_optim:\r\n",
      "      G.optim.load_state_dict(\r\n",
      "        torch.load('%s/%s.pth' % (root, join_strings('_', ['G_optim', name_suffix]))))\r\n",
      "  if D is not None:\r\n",
      "    D.load_state_dict(\r\n",
      "      torch.load('%s/%s.pth' % (root, join_strings('_', ['D', name_suffix]))),\r\n",
      "      strict=strict)\r\n",
      "    if load_optim:\r\n",
      "      D.optim.load_state_dict(\r\n",
      "        torch.load('%s/%s.pth' % (root, join_strings('_', ['D_optim', name_suffix]))))\r\n",
      "  # Load state dict\r\n",
      "  for item in state_dict:\r\n",
      "    state_dict[item] = torch.load('%s/%s.pth' % (root, join_strings('_', ['state_dict', name_suffix])))[item]\r\n",
      "  if G_ema is not None:\r\n",
      "    G_ema.load_state_dict(\r\n",
      "      torch.load('%s/%s.pth' % (root, join_strings('_', ['G_ema', name_suffix]))),\r\n",
      "      strict=strict)\r\n",
      "\r\n",
      "\r\n",
      "''' MetricsLogger originally stolen from VoxNet source code.\r\n",
      "    Used for logging inception metrics'''\r\n",
      "class MetricsLogger(object):\r\n",
      "  def __init__(self, fname, reinitialize=False):\r\n",
      "    self.fname = fname\r\n",
      "    self.reinitialize = reinitialize\r\n",
      "    if os.path.exists(self.fname):\r\n",
      "      if self.reinitialize:\r\n",
      "        print('{} exists, deleting...'.format(self.fname))\r\n",
      "        os.remove(self.fname)\r\n",
      "\r\n",
      "  def log(self, record=None, **kwargs):\r\n",
      "    \"\"\"\r\n",
      "    Assumption: no newlines in the input.\r\n",
      "    \"\"\"\r\n",
      "    if record is None:\r\n",
      "      record = {}\r\n",
      "    record.update(kwargs)\r\n",
      "    record['_stamp'] = time.time()\r\n",
      "    with open(self.fname, 'a') as f:\r\n",
      "      f.write(json.dumps(record, ensure_ascii=True) + '\\n')\r\n",
      "\r\n",
      "\r\n",
      "# Logstyle is either:\r\n",
      "# '%#.#f' for floating point representation in text\r\n",
      "# '%#.#e' for exponent representation in text\r\n",
      "# 'npz' for output to npz # NOT YET SUPPORTED\r\n",
      "# 'pickle' for output to a python pickle # NOT YET SUPPORTED\r\n",
      "# 'mat' for output to a MATLAB .mat file # NOT YET SUPPORTED\r\n",
      "class MyLogger(object):\r\n",
      "  def __init__(self, fname, reinitialize=False, logstyle='%3.3f'):\r\n",
      "    self.root = fname\r\n",
      "    if not os.path.exists(self.root):\r\n",
      "      os.mkdir(self.root)\r\n",
      "    self.reinitialize = reinitialize\r\n",
      "    self.metrics = []\r\n",
      "    self.logstyle = logstyle # One of '%3.3f' or like '%3.3e'\r\n",
      "\r\n",
      "  # Delete log if re-starting and log already exists\r\n",
      "  def reinit(self, item):\r\n",
      "    if os.path.exists('%s/%s.log' % (self.root, item)):\r\n",
      "      if self.reinitialize:\r\n",
      "        # Only print the removal mess\r\n",
      "        if 'sv' in item :\r\n",
      "          if not any('sv' in item for item in self.metrics):\r\n",
      "            print('Deleting singular value logs...')\r\n",
      "        else:\r\n",
      "          print('{} exists, deleting...'.format('%s_%s.log' % (self.root, item)))\r\n",
      "        os.remove('%s/%s.log' % (self.root, item))\r\n",
      "  \r\n",
      "  # Log in plaintext; this is designed for being read in MATLAB(sorry not sorry)\r\n",
      "  def log(self, itr, **kwargs):\r\n",
      "    for arg in kwargs:\r\n",
      "      if arg not in self.metrics:\r\n",
      "        if self.reinitialize:\r\n",
      "          self.reinit(arg)\r\n",
      "        self.metrics += [arg]\r\n",
      "      if self.logstyle == 'pickle':\r\n",
      "        print('Pickle not currently supported...')\r\n",
      "         # with open('%s/%s.log' % (self.root, arg), 'a') as f:\r\n",
      "          # pickle.dump(kwargs[arg], f)\r\n",
      "      elif self.logstyle == 'mat':\r\n",
      "        print('.mat logstyle not currently supported...')\r\n",
      "      else:\r\n",
      "        with open('%s/%s.log' % (self.root, arg), 'a') as f:\r\n",
      "          f.write('%d: %s\\n' % (itr, self.logstyle % kwargs[arg]))\r\n",
      "\r\n",
      "\r\n",
      "# Write some metadata to the logs directory\r\n",
      "def write_metadata(logs_root, experiment_name, config, state_dict):\r\n",
      "  with open(('%s/%s/metalog.txt' % \r\n",
      "             (logs_root, experiment_name)), 'w') as writefile:\r\n",
      "    writefile.write('datetime: %s\\n' % str(datetime.datetime.now()))\r\n",
      "    writefile.write('config: %s\\n' % str(config))\r\n",
      "    writefile.write('state: %s\\n' %str(state_dict))\r\n",
      "\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "Very basic progress indicator to wrap an iterable in.\r\n",
      "\r\n",
      "Author: Jan Schlter\r\n",
      "Andy's adds: time elapsed in addition to ETA, makes it possible to add\r\n",
      "estimated time to 1k iters instead of estimated time to completion.\r\n",
      "\"\"\"\r\n",
      "def progress(items, desc='', total=None, min_delay=0.1, displaytype='s1k'):\r\n",
      "  \"\"\"\r\n",
      "  Returns a generator over `items`, printing the number and percentage of\r\n",
      "  items processed and the estimated remaining processing time before yielding\r\n",
      "  the next item. `total` gives the total number of items (required if `items`\r\n",
      "  has no length), and `min_delay` gives the minimum time in seconds between\r\n",
      "  subsequent prints. `desc` gives an optional prefix text (end with a space).\r\n",
      "  \"\"\"\r\n",
      "  total = total or len(items)\r\n",
      "  t_start = time.time()\r\n",
      "  t_last = 0\r\n",
      "  for n, item in enumerate(items):\r\n",
      "    t_now = time.time()\r\n",
      "    if t_now - t_last > min_delay:\r\n",
      "      print(\"\\r%s%d/%d (%6.2f%%)\" % (\r\n",
      "              desc, n+1, total, n / float(total) * 100), end=\" \")\r\n",
      "      if n > 0:\r\n",
      "        \r\n",
      "        if displaytype == 's1k': # minutes/seconds for 1000 iters\r\n",
      "          next_1000 = n + (1000 - n%1000)\r\n",
      "          t_done = t_now - t_start\r\n",
      "          t_1k = t_done / n * next_1000\r\n",
      "          outlist = list(divmod(t_done, 60)) + list(divmod(t_1k - t_done, 60))\r\n",
      "          print(\"(TE/ET1k: %d:%02d / %d:%02d)\" % tuple(outlist), end=\" \")\r\n",
      "        else:# displaytype == 'eta':\r\n",
      "          t_done = t_now - t_start\r\n",
      "          t_total = t_done / n * total\r\n",
      "          outlist = list(divmod(t_done, 60)) + list(divmod(t_total - t_done, 60))\r\n",
      "          print(\"(TE/ETA: %d:%02d / %d:%02d)\" % tuple(outlist), end=\" \")\r\n",
      "          \r\n",
      "      sys.stdout.flush()\r\n",
      "      t_last = t_now\r\n",
      "    yield item\r\n",
      "  t_total = time.time() - t_start\r\n",
      "  print(\"\\r%s%d/%d (100.00%%) (took %d:%02d)\" % ((desc, total, total) +\r\n",
      "                                                   divmod(t_total, 60)))\r\n",
      "\r\n",
      "\r\n",
      "# Sample function for use with inception metrics\r\n",
      "def sample(G, z_, y_, config):\r\n",
      "  with torch.no_grad():\r\n",
      "    z_.sample_()\r\n",
      "    y_.sample_()\r\n",
      "    if config['parallel']:\r\n",
      "      G_z =  nn.parallel.data_parallel(G, (z_, G.shared(y_)))\r\n",
      "    else:\r\n",
      "      G_z = G(z_, G.shared(y_))\r\n",
      "    return G_z, y_\r\n",
      "\r\n",
      "\r\n",
      "# Sample function for sample sheets\r\n",
      "def sample_sheet(G, classes_per_sheet, num_classes, samples_per_class, parallel,\r\n",
      "                 samples_root, experiment_name, folder_number, z_=None):\r\n",
      "  # Prepare sample directory\r\n",
      "  if not os.path.isdir('%s/%s' % (samples_root, experiment_name)):\r\n",
      "    os.mkdir('%s/%s' % (samples_root, experiment_name))\r\n",
      "  if not os.path.isdir('%s/%s/%d' % (samples_root, experiment_name, folder_number)):\r\n",
      "    os.mkdir('%s/%s/%d' % (samples_root, experiment_name, folder_number))\r\n",
      "  # loop over total number of sheets\r\n",
      "  for i in range(num_classes // classes_per_sheet):\r\n",
      "    ims = []\r\n",
      "    y = torch.arange(i * classes_per_sheet, (i + 1) * classes_per_sheet, device='cuda')\r\n",
      "    for j in range(samples_per_class):\r\n",
      "      if (z_ is not None) and hasattr(z_, 'sample_') and classes_per_sheet <= z_.size(0):\r\n",
      "        z_.sample_()\r\n",
      "      else:\r\n",
      "        z_ = torch.randn(classes_per_sheet, G.dim_z, device='cuda')        \r\n",
      "      with torch.no_grad():\r\n",
      "        if parallel:\r\n",
      "          o = nn.parallel.data_parallel(G, (z_[:classes_per_sheet], G.shared(y)))\r\n",
      "        else:\r\n",
      "          o = G(z_[:classes_per_sheet], G.shared(y))\r\n",
      "\r\n",
      "      ims += [o.data.cpu()]\r\n",
      "    # This line should properly unroll the images\r\n",
      "    out_ims = torch.stack(ims, 1).view(-1, ims[0].shape[1], ims[0].shape[2], \r\n",
      "                                       ims[0].shape[3]).data.float().cpu()\r\n",
      "    # The path for the samples\r\n",
      "    image_filename = '%s/%s/%d/samples%d.jpg' % (samples_root, experiment_name, \r\n",
      "                                                 folder_number, i)\r\n",
      "    torchvision.utils.save_image(out_ims, image_filename,\r\n",
      "                                 nrow=samples_per_class, normalize=True)\r\n",
      "\r\n",
      "\r\n",
      "# Interp function; expects x0 and x1 to be of shape (shape0, 1, rest_of_shape..)\r\n",
      "def interp(x0, x1, num_midpoints):\r\n",
      "  lerp = torch.linspace(0, 1.0, num_midpoints + 2, device='cuda').to(x0.dtype)\r\n",
      "  return ((x0 * (1 - lerp.view(1, -1, 1))) + (x1 * lerp.view(1, -1, 1)))\r\n",
      "\r\n",
      "\r\n",
      "# interp sheet function\r\n",
      "# Supports full, class-wise and intra-class interpolation\r\n",
      "def interp_sheet(G, num_per_sheet, num_midpoints, num_classes, parallel,\r\n",
      "                 samples_root, experiment_name, folder_number, sheet_number=0,\r\n",
      "                 fix_z=False, fix_y=False, device='cuda'):\r\n",
      "  # Prepare zs and ys\r\n",
      "  if fix_z: # If fix Z, only sample 1 z per row\r\n",
      "    zs = torch.randn(num_per_sheet, 1, G.dim_z, device=device)\r\n",
      "    zs = zs.repeat(1, num_midpoints + 2, 1).view(-1, G.dim_z)\r\n",
      "  else:\r\n",
      "    zs = interp(torch.randn(num_per_sheet, 1, G.dim_z, device=device),\r\n",
      "                torch.randn(num_per_sheet, 1, G.dim_z, device=device),\r\n",
      "                num_midpoints).view(-1, G.dim_z)\r\n",
      "  if fix_y: # If fix y, only sample 1 z per row\r\n",
      "    ys = sample_1hot(num_per_sheet, num_classes)\r\n",
      "    ys = G.shared(ys).view(num_per_sheet, 1, -1)\r\n",
      "    ys = ys.repeat(1, num_midpoints + 2, 1).view(num_per_sheet * (num_midpoints + 2), -1)\r\n",
      "  else:\r\n",
      "    ys = interp(G.shared(sample_1hot(num_per_sheet, num_classes)).view(num_per_sheet, 1, -1),\r\n",
      "                G.shared(sample_1hot(num_per_sheet, num_classes)).view(num_per_sheet, 1, -1),\r\n",
      "                num_midpoints).view(num_per_sheet * (num_midpoints + 2), -1)\r\n",
      "  # Run the net--note that we've already passed y through G.shared.\r\n",
      "  if G.fp16:\r\n",
      "    zs = zs.half()\r\n",
      "  with torch.no_grad():\r\n",
      "    if parallel:\r\n",
      "      out_ims = nn.parallel.data_parallel(G, (zs, ys)).data.cpu()\r\n",
      "    else:\r\n",
      "      out_ims = G(zs, ys).data.cpu()\r\n",
      "  interp_style = '' + ('Z' if not fix_z else '') + ('Y' if not fix_y else '')\r\n",
      "  image_filename = '%s/%s/%d/interp%s%d.jpg' % (samples_root, experiment_name,\r\n",
      "                                                folder_number, interp_style,\r\n",
      "                                                sheet_number)\r\n",
      "  torchvision.utils.save_image(out_ims, image_filename,\r\n",
      "                               nrow=num_midpoints + 2, normalize=True)\r\n",
      "\r\n",
      "\r\n",
      "# Convenience debugging function to print out gradnorms and shape from each layer\r\n",
      "# May need to rewrite this so we can actually see which parameter is which\r\n",
      "def print_grad_norms(net):\r\n",
      "    gradsums = [[float(torch.norm(param.grad).item()),\r\n",
      "                 float(torch.norm(param).item()), param.shape]\r\n",
      "                for param in net.parameters()]\r\n",
      "    order = np.argsort([item[0] for item in gradsums])\r\n",
      "    print(['%3.3e,%3.3e, %s' % (gradsums[item_index][0],\r\n",
      "                                gradsums[item_index][1],\r\n",
      "                                str(gradsums[item_index][2])) \r\n",
      "                              for item_index in order])\r\n",
      "\r\n",
      "\r\n",
      "# Get singular values to log. This will use the state dict to find them\r\n",
      "# and substitute underscores for dots.\r\n",
      "def get_SVs(net, prefix):\r\n",
      "  d = net.state_dict()\r\n",
      "  return {('%s_%s' % (prefix, key)).replace('.', '_') :\r\n",
      "            float(d[key].item())\r\n",
      "            for key in d if 'sv' in key}\r\n",
      "\r\n",
      "\r\n",
      "# Name an experiment based on its config\r\n",
      "def name_from_config(config):\r\n",
      "  if config['mrq'] != None:\r\n",
      "    mr_string = 'mrq%.2f' % config['mrq']\r\n",
      "  else:\r\n",
      "    mr_string = 'mrt%.2f' % config['mrt']\r\n",
      "  name = '_'.join([\r\n",
      "  item for item in [\r\n",
      "  'Big%s' % config['which_train_fn'],\r\n",
      "  config['dataset'],\r\n",
      "  config['model'] if config['model'] != 'BigGAN' else None,\r\n",
      "  mr_string,\r\n",
      "  'seed%d' % config['seed'],\r\n",
      "  'Gch%d' % config['G_ch'],\r\n",
      "  'Dch%d' % config['D_ch'],\r\n",
      "  'Gd%d' % config['G_depth'] if config['G_depth'] > 1 else None,\r\n",
      "  'Dd%d' % config['D_depth'] if config['D_depth'] > 1 else None,\r\n",
      "  'bs%d' % config['batch_size'],\r\n",
      "  'Gfp16' if config['G_fp16'] else None,\r\n",
      "  'Dfp16' if config['D_fp16'] else None,\r\n",
      "  'nDs%d' % config['num_D_steps'] if config['num_D_steps'] > 1 else None,\r\n",
      "  'nDa%d' % config['num_D_accumulations'] if config['num_D_accumulations'] > 1 else None,\r\n",
      "  'nGa%d' % config['num_G_accumulations'] if config['num_G_accumulations'] > 1 else None,\r\n",
      "  'Glr%2.1e' % config['G_lr'],\r\n",
      "  'Dlr%2.1e' % config['D_lr'],\r\n",
      "  'GB%3.3f' % config['G_B1'] if config['G_B1'] !=0.0 else None,\r\n",
      "  'GBB%3.3f' % config['G_B2'] if config['G_B2'] !=0.999 else None,\r\n",
      "  'DB%3.3f' % config['D_B1'] if config['D_B1'] !=0.0 else None,\r\n",
      "  'DBB%3.3f' % config['D_B2'] if config['D_B2'] !=0.999 else None,\r\n",
      "  'Gnl%s' % config['G_nl'],\r\n",
      "  'Dnl%s' % config['D_nl'],\r\n",
      "  'Ginit%s' % config['G_init'],\r\n",
      "  'Dinit%s' % config['D_init'],\r\n",
      "  'G%s' % config['G_param'] if config['G_param'] != 'SN' else None,\r\n",
      "  'D%s' % config['D_param'] if config['D_param'] != 'SN' else None,\r\n",
      "  'Gattn%s' % config['G_attn'] if config['G_attn'] != '0' else None,\r\n",
      "  'Dattn%s' % config['D_attn'] if config['D_attn'] != '0' else None,\r\n",
      "  'Gortho%2.1e' % config['G_ortho'] if config['G_ortho'] > 0.0 else None,\r\n",
      "  'Dortho%2.1e' % config['D_ortho'] if config['D_ortho'] > 0.0 else None,\r\n",
      "  config['norm_style'] if config['norm_style'] != 'bn' else None,\r\n",
      "  'cr' if config['cross_replica'] else None,\r\n",
      "  'Gshared' if config['G_shared'] else None,\r\n",
      "  'hier' if config['hier'] else None,\r\n",
      "  'ema' if config['ema'] else None,\r\n",
      "  config['name_suffix'] if config['name_suffix'] else None,\r\n",
      "  ]\r\n",
      "  if item is not None])\r\n",
      "  # dogball\r\n",
      "  if config['hashname']:\r\n",
      "    return hashname(name)\r\n",
      "  else:\r\n",
      "    return name\r\n",
      "\r\n",
      "\r\n",
      "# A simple function to produce a unique experiment name from the animal hashes.\r\n",
      "def hashname(name):\r\n",
      "  h = hash(name)\r\n",
      "  a = h % len(animal_hash.a)\r\n",
      "  h = h // len(animal_hash.a)\r\n",
      "  b = h % len(animal_hash.b)\r\n",
      "  h = h // len(animal_hash.c)\r\n",
      "  c = h % len(animal_hash.c)\r\n",
      "  return animal_hash.a[a] + animal_hash.b[b] + animal_hash.c[c]\r\n",
      "\r\n",
      "\r\n",
      "# Get GPU memory, -i is the index\r\n",
      "def query_gpu(indices):\r\n",
      "  os.system('nvidia-smi -i 0 --query-gpu=memory.free --format=csv')\r\n",
      "\r\n",
      "\r\n",
      "# Convenience function to count the number of parameters in a module\r\n",
      "def count_parameters(module):\r\n",
      "  print('Number of parameters: {}'.format(\r\n",
      "    sum([p.data.nelement() for p in module.parameters()])))\r\n",
      "\r\n",
      "\r\n",
      "# Convenience function to sample an index, not actually a 1-hot\r\n",
      "def sample_1hot(batch_size, num_classes, device='cuda'):\r\n",
      "  return torch.randint(low=0, high=num_classes, size=(batch_size,),\r\n",
      "          device=device, dtype=torch.int64, requires_grad=False)\r\n",
      "\r\n",
      "\r\n",
      "# A highly simplified convenience class for sampling from distributions\r\n",
      "# One could also use PyTorch's inbuilt distributions package.\r\n",
      "# Note that this class requires initialization to proceed as\r\n",
      "# x = Distribution(torch.randn(size))\r\n",
      "# x.init_distribution(dist_type, **dist_kwargs)\r\n",
      "# x = x.to(device,dtype)\r\n",
      "# This is partially based on https://discuss.pytorch.org/t/subclassing-torch-tensor/23754/2\r\n",
      "class Distribution(torch.Tensor):\r\n",
      "  # Init the params of the distribution\r\n",
      "  def init_distribution(self, dist_type, **kwargs):    \r\n",
      "    self.dist_type = dist_type\r\n",
      "    self.dist_kwargs = kwargs\r\n",
      "    if self.dist_type == 'normal':\r\n",
      "      self.mean, self.var = kwargs['mean'], kwargs['var']\r\n",
      "    elif self.dist_type == 'categorical':\r\n",
      "      self.num_categories = kwargs['num_categories']\r\n",
      "    elif self.dist_type == 'constant':\r\n",
      "      self.n_class = kwargs['n_class']\r\n",
      "\r\n",
      "  def sample_(self):\r\n",
      "    if self.dist_type == 'normal':\r\n",
      "      self.normal_(self.mean, self.var)\r\n",
      "    elif self.dist_type == 'categorical':\r\n",
      "      self.random_(0, self.num_categories)    \r\n",
      "    elif self.dist_type == 'constant':\r\n",
      "      self.fill_(self.n_class)\r\n",
      "    # return self.variable\r\n",
      "    \r\n",
      "  # Silly hack: overwrite the to() method to wrap the new object\r\n",
      "  # in a distribution as well\r\n",
      "  def to(self, *args, **kwargs):\r\n",
      "    new_obj = Distribution(self)\r\n",
      "    new_obj.init_distribution(self.dist_type, **self.dist_kwargs)\r\n",
      "    new_obj.data = super().to(*args, **kwargs)    \r\n",
      "    return new_obj\r\n",
      "\r\n",
      "\r\n",
      "# Convenience function to prepare a z and y vector\r\n",
      "def prepare_z_y(G_batch_size, dim_z, nclasses, device='cuda', \r\n",
      "                fp16=False,z_var=1.0):\r\n",
      "  z_ = Distribution(torch.randn(G_batch_size, dim_z, requires_grad=False))\r\n",
      "  z_.init_distribution('normal', mean=0, var=z_var)\r\n",
      "  z_ = z_.to(device,torch.float16 if fp16 else torch.float32)   \r\n",
      "  \r\n",
      "  if fp16:\r\n",
      "    z_ = z_.half()\r\n",
      "\r\n",
      "  y_ = Distribution(torch.zeros(G_batch_size, requires_grad=False))\r\n",
      "  y_.init_distribution('categorical',num_categories=nclasses)\r\n",
      "  y_ = y_.to(device, torch.int64)\r\n",
      "  return z_, y_\r\n",
      "\r\n",
      "\r\n",
      "def initiate_standing_stats(net):\r\n",
      "  for module in net.modules():\r\n",
      "    if hasattr(module, 'accumulate_standing'):\r\n",
      "      module.reset_stats()\r\n",
      "      module.accumulate_standing = True\r\n",
      "\r\n",
      "\r\n",
      "def accumulate_standing_stats(net, z, y, nclasses, num_accumulations=16):\r\n",
      "  initiate_standing_stats(net)\r\n",
      "  net.train()\r\n",
      "  for i in range(num_accumulations):\r\n",
      "    with torch.no_grad():\r\n",
      "      z.normal_()\r\n",
      "      y.random_(0, nclasses)\r\n",
      "      x = net(z, net.shared(y)) # No need to parallelize here unless using syncbn\r\n",
      "  # Set to eval mode\r\n",
      "  net.eval() \r\n",
      "\r\n",
      "\r\n",
      "# This version of Adam keeps an fp32 copy of the parameters and\r\n",
      "# does all of the parameter updates in fp32, while still doing the\r\n",
      "# forwards and backwards passes using fp16 (i.e. fp16 copies of the\r\n",
      "# parameters and fp16 activations).\r\n",
      "#\r\n",
      "# Note that this calls .float().cuda() on the params.\r\n",
      "import math\r\n",
      "from torch.optim.optimizer import Optimizer\r\n",
      "class Adam16(Optimizer):\r\n",
      "  def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,weight_decay=0):\r\n",
      "    defaults = dict(lr=lr, betas=betas, eps=eps,\r\n",
      "            weight_decay=weight_decay)\r\n",
      "    params = list(params)\r\n",
      "    super(Adam16, self).__init__(params, defaults)\r\n",
      "      \r\n",
      "  # Safety modification to make sure we floatify our state\r\n",
      "  def load_state_dict(self, state_dict):\r\n",
      "    super(Adam16, self).load_state_dict(state_dict)\r\n",
      "    for group in self.param_groups:\r\n",
      "      for p in group['params']:\r\n",
      "        self.state[p]['exp_avg'] = self.state[p]['exp_avg'].float()\r\n",
      "        self.state[p]['exp_avg_sq'] = self.state[p]['exp_avg_sq'].float()\r\n",
      "        self.state[p]['fp32_p'] = self.state[p]['fp32_p'].float()\r\n",
      "\r\n",
      "  def step(self, closure=None):\r\n",
      "    \"\"\"Performs a single optimization step.\r\n",
      "    Arguments:\r\n",
      "      closure (callable, optional): A closure that reevaluates the model\r\n",
      "        and returns the loss.\r\n",
      "    \"\"\"\r\n",
      "    loss = None\r\n",
      "    if closure is not None:\r\n",
      "      loss = closure()\r\n",
      "\r\n",
      "    for group in self.param_groups:\r\n",
      "      for p in group['params']:\r\n",
      "        if p.grad is None:\r\n",
      "          continue\r\n",
      "          \r\n",
      "        grad = p.grad.data.float()\r\n",
      "        state = self.state[p]\r\n",
      "\r\n",
      "        # State initialization\r\n",
      "        if len(state) == 0:\r\n",
      "          state['step'] = 0\r\n",
      "          # Exponential moving average of gradient values\r\n",
      "          state['exp_avg'] = grad.new().resize_as_(grad).zero_()\r\n",
      "          # Exponential moving average of squared gradient values\r\n",
      "          state['exp_avg_sq'] = grad.new().resize_as_(grad).zero_()\r\n",
      "          # Fp32 copy of the weights\r\n",
      "          state['fp32_p'] = p.data.float()\r\n",
      "\r\n",
      "        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\r\n",
      "        beta1, beta2 = group['betas']\r\n",
      "\r\n",
      "        state['step'] += 1\r\n",
      "\r\n",
      "        if group['weight_decay'] != 0:\r\n",
      "          grad = grad.add(group['weight_decay'], state['fp32_p'])\r\n",
      "\r\n",
      "        # Decay the first and second moment running average coefficient\r\n",
      "        exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n",
      "        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n",
      "\r\n",
      "        denom = exp_avg_sq.sqrt().add_(group['eps'])\r\n",
      "\r\n",
      "        bias_correction1 = 1 - beta1 ** state['step']\r\n",
      "        bias_correction2 = 1 - beta2 ** state['step']\r\n",
      "        step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\r\n",
      "      \r\n",
      "        state['fp32_p'].addcdiv_(-step_size, exp_avg, denom)\r\n",
      "        p.data = state['fp32_p'].half()\r\n",
      "\r\n",
      "    return loss\r\n"
     ]
    }
   ],
   "source": [
    "%cat /home/cybai/BigGAN-PyTorch/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-conjunction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
